<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>ATD Scan report</title>
    <link href="https://fonts.googleapis.com/css?family=Bitter:400,700" rel="stylesheet">
    <style>
        body {
          margin: 0;
          padding: 0;
          background-color: #cccccc;
          color: #333333;
          font-size: 15px;
          line-height: 2;
        }

        p, h1, h2, h3, h4, h5, h6 {
          margin-top: 0;
        }

        img {
          vertical-align: bottom;
        }

        ul {
          margin: 0;
          padding: 0;
        }

        a {
          color: #3583aa;
          text-decoration: none;
        }

        a:visited {
          color: #788d98;
        }

        a:hover {
          text-decoration: underline;
        }

        header {
          width: 960px;
          height: 100px;
          margin: 0 auto;
        }

        .logo {
          float: left;
          margin-top: 50px;
        }

        .global-nav {
          float: right;
          margin-top: 60px;
        }

        .global-nav li {
          float: left;
          margin: 0 30px;
          font-size: 20px;
          list-style: none;
          font-family: 'Bitter', serif;
        }

        .global-nav li a {
          color: #ffffff;
        }

        .global-nav li a:hover {
          border-bottom: 2px solid: #ffffff;
          padding-bottom: 3px;
          text-decoration: none;
        }

        #wrap {
          clear: both;
          background-color: #ffffff;
          margin-top: 100px;
          padding: 40px 0;
        }

        .content {
          width: 960px;
          margin: 0 auto;
        }

        footer {
          text-align: center;
          color: #ffffff;
          padding: 20px 0;
          background-color: #767671;
        }

        footer small {
          font-size: 12px;
        }

        #about {
          background-image: url(../img/background.jpg);
          background-repeat: no-repeat;
          background-position: center top;
          background-attachment: fixed;
          background-size: 100% auto;
        }

        .main-center {
          width: 940px;
          margin: 0 auto;
        }

        h1 {
          font-family: 'Bitter', serif;
          font-size: 36px;
          border-bottom: 1px solid #cccccc;
        }

        h2 {
          font-family: 'Bitter', serif;
          font-size: 24px;
        }

        h3 {
          font-family: 'Bitter', serif;
          font-size: 18px;
        }

        h4 {
          font-family: 'Bitter', serif;
          font-size: 16px;
        }

        .h2_icon:before {
          content: "";
          padding-right: 10px;
          border-left: 7px solid #9cb4a4;
        }

        .h3_icon:before {
          content: "";
          padding-right: 10px;
          border-left: 7px solid #696969;
        }

        #about .executive_summary {
          width: 940px;
        }

        #about .executive_summary span {
          font-weight: bold;
        }

        #about .clearfix:after {
          content: "";
          display: block;
          clear: both;
        }

        #about .table_target_info th {
          width: 940px;
          background-color: #f0f0f0;
          padding: 12px 0;
          border: 1px solid #cccccc;
        }

        #about .table_target_info td {
          width: 940px;
          border: 1px solid #cccccc;
          text-align: left;
          padding: 0 0 0 10px;
        }

        #about .table_target_info tr th:nth-child(1) {
          width: 200px;
          text-align: center;
          vertical-align: center;
        }

        #about .table_summary th {
          width: 940px;
          background-color: #f0f0f0;
          padding: 12px;
          text-align: center;
          border: 1px solid #cccccc;
        }

        #about .table_summary td {
          width: 940px;
          border: 1px solid #cccccc;
        }

        #about .table_summary tr td:nth-child(1) {
          width: 50px;
          text-align: center;
          vertical-align: center;
        }

        #about .table_summary tr td:nth-child(2) {
          width: 100px;
          text-align: center;
          vertical-align: center;
        }

        #about .table_summary tr td:nth-child(3) {
          width: 100px;
          text-align: center;
          padding: 12px;
          vertical-align: center;
        }

        #about .table_summary tr td:nth-child(4) {
          width: 500px;
          text-align: left;
          padding: 12px;
          vertical-align: center;
        }

        table {
          border-spacing: 0;
          border-collapse: collapse;
          margin: 0 0 20px 0;
          float: left;
        }

        section {
          margin-bottom: 35px;
        }

        section .target_information {
          text-align: left;
          padding: 0 0 0 10px;
        }

        section .dataset {
          text-align: left;
          padding: 0 0 0 10px;
        }

        section .summary {
          text-align: left;
          padding: 0 0 0 10px;
        }

        section .vuln_1st_category {
          text-align: left;
          padding: 0 0 0 10px;
        }

        section .vuln_1st_category ul, ol {
          padding: 0;
          position: relative;
          margin: 0 0 30px;
        }

        section .vuln_1st_category ul li, ol li {
          color: #e366a;
          border-left: solid 10px #cccccc;
          background: #f5f5f5;
          margin-bottom: 3px;
          line-height: 1.5;
          padding: 0.5em;
          list-style-type: none!important;
        }

        section .vuln_2nd_category {
          text-align: left;
          padding: 0 0 0 20px;
        }

        #about .benign_list li {
          float: left;
          list-style: none;
          margin: 0 20px 15px 0;
        }

        #about .aes_list li {
          float: left;
          list-style: none;
          margin: 0 20px 15px 0;
        }
    </style>
</head>

<body id="about">
<header>
    <div class="logo">
        <img src="../img/logo-white.png" width=250 alt="Adversarial Threat Detector's logo.">
    </div>
    <nav>
        <ul class="global-nav">
            <li><a href="scan_report.html#executive_summary">総評</a></li>
            <li><a href="scan_report.html#scanned_result">結果概要</a></li>
            <li><a href="scan_report.html#vulnerability_detail">脆弱性詳細</a></li>
        </ul>
    </nav>
</header>
<div id="wrap">
    <div class="content">
        <div class="main-center">
            <h1>ATD Scan Report</h1>
            <p>ATD (Adversarial Threat Detector) のスキャン・レポートです。</p>
            <section id="executive_summary" class="executive_summary clearfix">
                <div class="summary-txt">
                    <h2 class="h2_icon">総評</h2>
                    <p><span><font color="red">{{ target.rank }}</font></span><br><b>{{ target.summary }}</b></p>
                </div>
            </section>

            <section id="scanned_result" class="scanned_result">
                <h2 class="h2_icon">結果概要</h2>

                <section class="target_information">
                    <h3 class="h3_icon">基本情報</h3>
                    <p>スキャン対象モデルの基本情報です。</p>
                    <table class="table_target_info">
                        <tr>
                            <th>モデル</th>
                            <td>{{ target.model_path }}</td>
                        </tr>
                        <tr>
                            <th>データセット</th>
                            <td>{{ target.dataset_path }}</td>
                        </tr>
                        <tr>
                            <th>データ数</th>
                            <td>{{ target.dataset_num }}</td>
                        </tr>
                        <tr>
                            <th>推論精度</th>
                            <td>{{ target.accuracy }}</td>
                        </tr>
                    </table>
                </section>

                <section class="dataset">
                    <h3 class="h3_icon">データセット</h3>
                    <p>データセットの一例を表示します。</p>
                    <ul class="clearfix benign_list">
                        <li><img src="{{ target.dataset_img.img1 }}" alt="Benign sample #1."></li>
                        <li><img src="{{ target.dataset_img.img2 }}" alt="Benign sample #2."></li>
                        <li><img src="{{ target.dataset_img.img3 }}" alt="Benign sample #3."></li>
                        <li><img src="{{ target.dataset_img.img4 }}" alt="Benign sample #4."></li>
                        <li><img src="{{ target.dataset_img.img5 }}" alt="Benign sample #5."></li>
                    </ul>
                </section>

                <section class="summary">
                    <h3 class="h3_icon">結果概要</h3>
                    <p>スキャン結果一覧です。</p>
                    <table class="table_summary">
                        <thead>
                        <tr>
                            <th>#</th>
                            <th>攻撃手法</th>
                            <th>スキャン結果</th>
                            <th>スキャン結果概要</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td>1</td>
                            <td>データ汚染攻撃</td>
                            {% if data_poisoning.exist %}
                            <td>{{ data_poisoning.consequence }}</td>
                            <td>{{ data_poisoning.summary }}</td>
                            {% else %}
                            <td>N/A</td>
                            <td>未スキャン。</td>
                            {% endif %}
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>モデル汚染攻撃</td>
                            {% if model_poisoning.exist %}
                            <td>{{ model_poisoning.consequence }}</td>
                            <td>{{ model_poisoning.summary }}</td>
                            {% else %}
                            <td>N/A</td>
                            <td>未スキャン。</td>
                            {% endif %}
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>回避攻撃</td>
                            {% if evasion.exist %}
                            <td>{{ evasion.consequence }}</td>
                            <td>{{ evasion.summary }}</td>
                            {% else %}
                            <td>N/A</td>
                            <td>未スキャン。</td>
                            {% endif %}
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>抽出攻撃</td>
                            {% if exfiltration.exist %}
                            <td>{{ exfiltration.consequence }}</td>
                            <td>{{ exfiltration.summary }}</td>
                            {% else %}
                            <td>N/A</td>
                            <td>未スキャン。</td>
                            {% endif %}
                        </tr>
                        </tbody>
                    </table>
                </section>
            </section>

            <!-- 脆弱性の詳細情報 -->
            <section id="vulnerability_detail" class="vulnerability_detail">
                <h2 class="h2_icon">脆弱性詳細</h2>
                <p><span>開発者向けの詳細情報です。</span><br>以下の脆弱性情報を基に、対策を行ってください。</p>

                <section class="vuln_1st_category">
                    {% if data_poisoning.exist %}
                    <!-- データ汚染攻撃の詳細情報 -->
                    <h3 class="h3_icon">データ汚染攻撃</h3>
                    <p>データ汚染攻撃はAIにバックドアを設置する攻撃です。<br>攻撃者が標的AIの学習データに細工データを注入し、これを学習させることで、特定の入力データ（トリガー）を攻撃者が意図したクラスに誤分類するように決定境界が歪められます。</p>
                    <section class="vuln_2nd_category">
                        {% if data_poisoning.fc.exist %}
                        <h4>Feature Collision Attack</h4>
                            <p>Feature Collision Attack is a Perfect-knowledge attack that injects poison data that features approximate the Trigger into the train data of a target classifier.</p>
                        <ul>
                            <li>スキャン日時：{{ data_poisoning.fc.date }}</li>
                            <li>スキャン結果：{{ data_poisoning.fc.consequence }}</li>
                            <li>脆弱性の再現：{{ data_poisoning.fc.ipynb_path }}</li>
                            <li>対策　　　　：{{ data_poisoning.fc.countermeasure }}</li>
                        </ul>
                        {% endif %}

                        {% if data_poisoning.cp.exist %}
                        <h4>Convex Polytope Attack</h4>
                            <p>Convex Polytope Attack is a Zero-knowledge attack that poisoned train data of a target classifier so that the feature vector of the trigger lies within the convex polytope of the feature vectors of multiple poison data.</p>
                        <ul>
                            <li>スキャン日時：{{ data_poisoning.cp.date }}</li>
                            <li>スキャン結果：{{ data_poisoning.cp.consequence }}</li>
                            <li>脆弱性の再現：{{ data_poisoning.cp.ipynb_path }}</li>
                            <li>対策　　　　：{{ data_poisoning.cp.countermeasure }}</li>
                        </ul>
                        {% endif %}
                    </section>
                    {% endif %}

                    {% if model_poisoning.exist %}
                    <!-- モデル汚染攻撃の詳細情報 -->
                    <h3 class="h3_icon">モデル汚染攻撃</h3>
                    <p>Model Poisoning attack is an attack that embeds a "backdoor" and "malicious layers" into the target classifier.</p>
                    <p><b>The backdoor</b> causes the input data known only to the adversary (Trigger) to misclassify it into the class intended by the adversary. The adversary injects the Trojan Nodes into the hidden layers of the pre-trained model and distributes it to victim users. If victim user develop classifier using the pre-trained model injected the Trojan Nodes, then the decision boundary of the classifier is distorted by the Trojan nodes.</p>
                    <p><b>The malicious layers</b> execute the system commands intended by the adversary on the system operating the classifier. The adversary injects the layers describing arbitrary system commands (e.g. Lambda layer of TensorFlow) into the pre-trained model and distributes it to the victim users. If the victim users develop classifier using the pre-training model, the malicious layers execute the malicious system commands on the system operating the classifier.</p>
                    <section class="vuln_2nd_category">
                        {% if model_poisoning.node_injection.exist %}
                        <h4>Node Injection Attack</h4>
                            <p>Node Injection Attack is an attack that injects a Trojan Nodes into a pre-trained model that causes the Trigger to misclassify into the class intended by the adversary.</p>
                        <ul>
                            <li>スキャン日時：{{ model_poisoning.node_injection.date }}</li>
                            <li>スキャン結果：{{ model_poisoning.node_injection.consequence }}</li>
                            <li>脆弱性の再現：{{ model_poisoning.node_injection.ipynb_path }}</li>
                            <li>対策　　　　：{{ model_poisoning.node_injection.countermeasure }}</li>
                        </ul>
                        {% endif %}

                        {% if model_poisoning.layer_injection.exist %}
                        <h4>Malicious Layer Injection Attack</h4>
                            <p>Malicious Layer Injection Attack is an attack that injects layers that executes malicious system commands into a pre-trained model. If target classifier infers, then malicious system commands to be executed on the system operating the target classifier.</p>
                        <ul>
                            <li>スキャン日時：{{ model_poisoning.layer_injection.date }}</li>
                            <li>スキャン結果：{{ model_poisoning.layer_injection.consequence }}</li>
                            <li>脆弱性の再現：{{ model_poisoning.layer_injection.ipynb_path }}</li>
                            <li>対策　　　　：{{ model_poisoning.layer_injection.countermeasure }}</li>
                        </ul>
                        {% endif %}
                    </section>
                    {% endif %}

                    {% if evasion.exist %}
                    <!-- 回避攻撃の詳細情報 -->
                    <h3 class="h3_icon">回避攻撃</h3>
                    <p>回避攻撃は、分類器への入力データを攻撃者が意図したクラスに誤分類させる攻撃です。</p>
                    <p>攻撃者は分類器への入力データに摂動を加え、特徴量を変化させたデータ（敵対的サンプル）を作成します。そして、敵対的サンプルを攻撃対象の分類器に入力することで、これを攻撃者が意図したクラスに誤分類させることができます。</p>
                    <section class="vuln_2nd_category">
                        {% if evasion.fgsm.exist %}
                        <h4>Fast Gradient Sign Method (FGSM)</h4>
                            <p>FGSMは、分類器の勾配を利用して敵対的サンプルを作成する手法です。</p>
                            <p>例えば画像の場合、入力画像に対する損失関数の勾配を使用し、損失を大きくするように画像に摂動を加えていきます。</p>
                        <ul>
                            <li>スキャン日時　：{{ evasion.fgsm.date }}</li>
                            <li>スキャン結果　：{{ evasion.fgsm.consequence }}</li>
                            <li>脆弱性の再現　：{{ evasion.fgsm.ipynb_path }}</li>
                            <li>敵対的サンプル：{{ evasion.fgsm.aes_path }}</li>
                            <li>対策　　　　：{{ evasion.fgsm.countermeasure }}</li>
                        </ul>
                        <p>敵対的サンプルの一例。</p>
                        <ul class="clearfix aes_list">
                            <li><img src="{{ evasion.fgsm.ae_img.img1 }}" alt="Adversarial Exmaples #1."></li>
                            <li><img src="{{ evasion.fgsm.ae_img.img2 }}" alt="Adversarial Exmaples #2."></li>
                            <li><img src="{{ evasion.fgsm.ae_img.img3 }}" alt="Adversarial Exmaples #3."></li>
                            <li><img src="{{ evasion.fgsm.ae_img.img4 }}" alt="Adversarial Exmaples #4."></li>
                            <li><img src="{{ evasion.fgsm.ae_img.img5 }}" alt="Adversarial Exmaples #5."></li>
                        </ul>
                        {% endif %}

                        {% if evasion.cnw.exist %}
                        <h4>Carlini and Wagner Attack (C&W)</h4>
                            <p>C&W攻撃は、分類器のL2ノルムを使用して敵対的サンプルを作成します。</p>
                        <ul>
                            <li>スキャン日時　：{{ evasion.cnw.date }}</li>
                            <li>スキャン結果　：{{ evasion.cnw.consequence }}</li>
                            <li>脆弱性の再現　：{{ evasion.cnw.ipynb_path }}</li>
                            <li>敵対的サンプル：{{ evasion.cnw.aes_path }}</li>
                            <li>対策　　　　　：{{ evasion.cnw.countermeasure }}</li>
                        </ul>
                        <p>敵対的サンプルの一例。</p>
                        <ul class="clearfix aes_list">
                            <li><img src="{{ evasion.cnw.ae_img.img1 }}" alt="Adversarial Exmaples #1."></li>
                            <li><img src="{{ evasion.cnw.ae_img.img2 }}" alt="Adversarial Exmaples #2."></li>
                            <li><img src="{{ evasion.cnw.ae_img.img3 }}" alt="Adversarial Exmaples #3."></li>
                            <li><img src="{{ evasion.cnw.ae_img.img4 }}" alt="Adversarial Exmaples #4."></li>
                            <li><img src="{{ evasion.cnw.ae_img.img5 }}" alt="Adversarial Exmaples #5."></li>
                        </ul>
                        {% endif %}

                        {% if evasion.jsma.exist %}
                        <h4>Jacobian Saliency Map Attack (JSMA)</h4>
                            <p>JSMAは、分類器が誤分類器を引き起こす、または、加える摂動の上限を超えるまで、反復的に入力データに摂動を加えて敵対的サンプルを作成します。</p>
                        <ul>
                            <li>スキャン日時　：{{ evasion.jsma.date }}</li>
                            <li>スキャン結果　：{{ evasion.jsma.consequence }}</li>
                            <li>脆弱性の再現　：{{ evasion.jsma.ipynb_path }}</li>
                            <li>敵対的サンプル：{{ evasion.jsma.aes_path }}</li>
                            <li>対策　　　　：{{ evasion.jsma.countermeasure }}</li>
                        </ul>
                        <p>敵対的サンプルの一例。</p>
                        <ul class="clearfix aes_list">
                            <li><img src="{{ evasion.jsma.ae_img.img1 }}" alt="Adversarial Exmaples #1."></li>
                            <li><img src="{{ evasion.jsma.ae_img.img2 }}" alt="Adversarial Exmaples #2."></li>
                            <li><img src="{{ evasion.jsma.ae_img.img3 }}" alt="Adversarial Exmaples #3."></li>
                            <li><img src="{{ evasion.jsma.ae_img.img4 }}" alt="Adversarial Exmaples #4."></li>
                            <li><img src="{{ evasion.jsma.ae_img.img5 }}" alt="Adversarial Exmaples #5."></li>
                        </ul>
                        {% endif %}
                    </section>
                    {% endif %}

                    {% if exfiltration.exist %}
                    <!-- 抽出攻撃の詳細情報 -->
                    <h3 class="h3_icon">抽出攻撃</h3>
                    <p>Exfiltration is an attack that steals the parameters and train data of a target classifier.</p>
                    <section class="vuln_2nd_category">
                        {% if exfiltration.mi.exist %}
                        <h4>Membership Inference Attack</h4>
                            <p>Membership Inference Attack is an attack that the adversary infers the train data of the target classifier based on the input data and responses (classification labels and confidence values) to the target classifier.</p>
                        <ul>
                            <li>スキャン日時：{{ exfiltration.mi.date }}</li>
                            <li>スキャン結果：{{ exfiltration.mi.consequence }}</li>
                            <li>脆弱性の再現：{{ exfiltration.mi.ipynb_path }}</li>
                            <li>対策　　　　：{{ exfiltration.mi.countermeasure }}</li>
                        </ul>
                        {% endif %}

                        {% if exfiltration.label_only_mi.exist %}
                        <h4>Label Only Membership Inference Attack</h4>
                            <p>Label Only Membership Inference Attack is an attack that the adversary infers the train data of the target classifier based on the input data and response (classification labels) to the target classifier.</p>
                        <ul>
                            <li>スキャン日時：{{ exfiltration.label_only_mi.date }}</li>
                            <li>スキャン結果：{{ exfiltration.label_only_mi.consequence }}</li>
                            <li>脆弱性の再現：{{ exfiltration.label_only_mi.ipynb_path }}</li>
                            <li>対策　　　　：{{ exfiltration.label_only_mi.countermeasure }}</li>
                        </ul>
                        {% endif %}

                        {% if exfiltration.model_inversion.exist %}
                        <h4>Model Inversion	Attack</h4>
                            <p>Model extraction attack is an attack in that an adversary infers the decision boundary of a target classifier based on the input data and responses (classification labels and confidence values) to the target classifier.</p>
                        <ul>
                            <li>スキャン日時：{{ exfiltration.model_inversion.date }}</li>
                            <li>スキャン結果：{{ exfiltration.model_inversion.consequence }}</li>
                            <li>脆弱性の再現：{{ exfiltration.model_inversion.ipynb_path }}</li>
                            <li>対策　　　　：{{ exfiltration.model_inversion.countermeasure }}</li>
                        </ul>
                        {% endif %}
                    </section>
                    {% endif %}
                </section>
            </section>

        </div>
    </div>
</div>

<footer>
    <small>©2021- Mitsui Bussan Secure Directions, Inc. All rights reserved.</small>
</footer>
</body>
</html>